{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "888abe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import uproot\n",
    "import torch\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.cm as cm\n",
    "import ot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fd19a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "from sklearn.cluster import kmeans_plusplus\n",
    "\n",
    "def energy_weighted_kmeans(event, n_clusters=6, max_iter=100, tol=1e-4, plot=True, show_message=False):\n",
    "    \"\"\"\n",
    "    Performs energy-weighted K-Means clustering on particle production vertices.\n",
    "\n",
    "    Parameters:\n",
    "        event: awkward.Array    - Event data with true particle-level information.\n",
    "                                 Must contain fields: 'prod_x_true', 'prod_y_true', 'prod_z_true', 'p_true'\n",
    "        n_clusters: int         - Number of clusters to form (default: 6)\n",
    "        max_iter: int           - Maximum number of iterations for convergence (default: 100)\n",
    "        tol: float              - Convergence threshold on centroid shift (default: 1e-4)\n",
    "        plot: bool              - If True, plot 3D clustering result (default: True)\n",
    "        show_message: bool      - If True, print message upon convergence (default: False)\n",
    "\n",
    "    Behavior:\n",
    "        - Initializes centroids using KMeans++ from sklearn\n",
    "        - Assigns particles to nearest centroid based on Euclidean distance\n",
    "        - Updates centroids as energy-weighted mean of assigned points\n",
    "        - Reinitializes empty clusters with random points\n",
    "        - Stops iterating when centroid shift is below tolerance\n",
    "\n",
    "    Returns:\n",
    "        labels: ndarray (N,)         - Cluster label assigned to each particle\n",
    "        centroids: ndarray (n x 3)   - Final centroid positions in 3D space\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    np.random.seed(0)\n",
    "    x = ak.to_numpy(event['prod_x_true'])\n",
    "    y = ak.to_numpy(event['prod_y_true'])\n",
    "    z = ak.to_numpy(event['prod_z_true'])\n",
    "    \n",
    "    X = np.stack((x, y, z), axis=1)  # shape (N, 3)\n",
    "    # ! Not actually the energy, but momentum\n",
    "    energy = ak.to_numpy(event['p_true'])\n",
    "    \n",
    "    N = X.shape[0]\n",
    "\n",
    "    # Initialize centroids from points\n",
    "    centroids, _ = kmeans_plusplus(X, n_clusters=n_clusters, random_state=0)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # Assignment step (standard, unweighted distance)\n",
    "        labels = pairwise_distances_argmin(X, centroids)\n",
    "\n",
    "        # Update step (energy-weighted mean)\n",
    "        new_centroids = np.zeros_like(centroids)\n",
    "        for k in range(n_clusters):\n",
    "            mask = labels == k\n",
    "            if np.sum(mask) == 0:\n",
    "                # Reinitialize empty cluster to a random point\n",
    "                new_centroids[k] = X[np.random.randint(0, N)]\n",
    "            else:\n",
    "                weights = energy[mask]\n",
    "                points = X[mask]\n",
    "                new_centroids[k] = np.average(points, axis=0, weights=weights)\n",
    "\n",
    "        # Convergence check\n",
    "        shift = np.linalg.norm(centroids - new_centroids)\n",
    "        centroids = new_centroids\n",
    "        if shift < tol:\n",
    "            if show_message:\n",
    "                print(f\"Converged after {i} iterations\")\n",
    "            break\n",
    "            \n",
    "    if plot:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        scatter = ax.scatter(x, y, z, c=labels, cmap='plasma', s=10)\n",
    "        ax.scatter(centroids[:, 0], centroids[:, 1], centroids[:, 2], color='black', marker='x', s=100)\n",
    "        plt.title(\"Energy-Weighted K-Means Clustering\")\n",
    "        plt.show()\n",
    "    \n",
    "    return labels, centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd006752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ewc_from_clustered_event(event):\n",
    "    \"\"\"\n",
    "    Compute energy-weighted centroids from an event where particles are grouped by cluster,\n",
    "    and include total pt per cluster.\n",
    "\n",
    "    Parameters:\n",
    "        event: awkward.Record\n",
    "            - Must contain fields: 'cl_cell_xCells', 'cl_cell_yCells', 'cl_cell_zCells', 'cl_cell_pt'\n",
    "\n",
    "    Returns:\n",
    "        result: numpy array (n_clusters, 4)\n",
    "            - Columns: [x_centroid, y_centroid, z_centroid, total_pt]\n",
    "    \"\"\"\n",
    "    centroids_with_pt = []\n",
    "\n",
    "    n_clusters = len(event['cl_cell_xCells'])\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        total_pt = np.array(event['cl_pt'][i])\n",
    "        if total_pt < 1e4:\n",
    "            continue\n",
    "        x = ak.to_numpy(event['cl_cell_xCells'][i])\n",
    "        y = ak.to_numpy(event['cl_cell_yCells'][i])\n",
    "        z = ak.to_numpy(event['cl_cell_zCells'][i])\n",
    "        energy = ak.to_numpy(event['cl_cell_pt'][i])\n",
    "\n",
    "        if len(energy) == 0:\n",
    "            centroids_with_pt.append([np.nan, np.nan, np.nan, 0.0])\n",
    "            continue\n",
    "\n",
    "        points = np.stack((x, y, z), axis=1)\n",
    "        weighted_mean = np.average(points, axis=0, weights=energy)\n",
    "        #total_pt = np.sum(energy)\n",
    "\n",
    "        centroids_with_pt.append(list(weighted_mean) + [total_pt])\n",
    "\n",
    "    return np.array(centroids_with_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d172b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clustered_truth_array(truth_array):\n",
    "    \"\"\"\n",
    "    Applies energy-weighted K-Means clustering to each event and groups true particle features by cluster.\n",
    "\n",
    "    Parameters:\n",
    "        truth_array: awkward.Array - Array of events with particle-level truth information.\n",
    "                                        Each event must contain fields: \n",
    "                                        'prod_x_true', 'prod_y_true', 'prod_z_true',\n",
    "                                        'calo_x_true', 'calo_y_true', 'calo_z_true',\n",
    "                                        'px_true', 'py_true', 'pz_true',\n",
    "                                        'p_true', 'times_ATLAS_true', 'n_true'\n",
    "\n",
    "    Behavior:\n",
    "        - Runs energy-weighted K-Means clustering on each event with `n_clusters` clusters\n",
    "        - Extracts a defined set of fields per cluster\n",
    "        - Computes transverse momentum (pt = sqrt(px² + py²)) per particle and stores it as 'cell_pt'\n",
    "        - Skips clusters with no assigned particles\n",
    "        - Aggregates results into a list of structured dictionaries, one per event\n",
    "\n",
    "    Returns:\n",
    "        clustered_truth_array: awkward.Array - Structured array of clustered particle data per event,\n",
    "                                                including cluster-wise field groupings and per-cluster pt\n",
    "    \"\"\"\n",
    "\n",
    "    n_clusters = 6\n",
    "    clustered_truth_particles = []\n",
    "\n",
    "    # Fields to extract and group per cluster\n",
    "    cluster_fields = [\n",
    "        'prod_x_true', 'prod_y_true', 'prod_z_true',\n",
    "        'calo_x_true', 'calo_y_true', 'calo_z_true',\n",
    "        'px_true', 'py_true', 'pz_true',\n",
    "        'p_true', 'times_ATLAS_true'\n",
    "    ]\n",
    "\n",
    "    for event_idx, event in enumerate(truth_array):\n",
    "        labels, centroids = energy_weighted_kmeans(event, n_clusters=n_clusters, plot=False, show_message=False)\n",
    "\n",
    "        # Initialize field containers\n",
    "        clustered_data = {field: [] for field in cluster_fields}\n",
    "        cell_pt = []\n",
    "        cluster_pt = []\n",
    "        average_time = []\n",
    "\n",
    "        for cluster_id in range(n_clusters):\n",
    "            mask = labels == cluster_id\n",
    "            if np.sum(mask) == 0:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                for field in cluster_fields:\n",
    "                    clustered_data[field].append(event[field][mask])\n",
    "            except IndexError:\n",
    "                print(f\"Skipping inconsistent event at index {event_idx}\")\n",
    "                continue\n",
    "\n",
    "            # Compute transverse momentum (pt) per particle in this cluster\n",
    "            px = event['px_true'][mask]\n",
    "            py = event['py_true'][mask]\n",
    "            times = event['times_ATLAS_true'][mask]\n",
    "\n",
    "            # ? Should I use p instead of pt?\n",
    "            pt = np.sqrt(px**2 + py**2)\n",
    "            cell_pt.append(pt)\n",
    "            cluster_pt.append(np.sum(pt))\n",
    "            average_time.append(np.mean(times))\n",
    "\n",
    "    # ! I should add the time average for the cluster here and then fix the last cell\n",
    "        clustered_truth_particles.append({\n",
    "            'n_true': event['n_true'],\n",
    "            **clustered_data,\n",
    "            'cell_pt': cell_pt,\n",
    "            'centroid_x': [c[0] for c in centroids],\n",
    "            'centroid_y': [c[1] for c in centroids],\n",
    "            'centroid_z': [c[2] for c in centroids],\n",
    "            'cl_pt': cluster_pt,\n",
    "            'cl_time': average_time\n",
    "        })\n",
    "        # är det inte konstigt att den convergar med så få iteratiorner?\n",
    "    return ak.Array(clustered_truth_particles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9849d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_awkward(arr, features=(\"x\", \"y\", \"z\", \"pt\")):\n",
    "    \"\"\"\n",
    "    Converts an Awkward Array of objects with specified feature keys into a NumPy array.\n",
    "\n",
    "    Parameters:\n",
    "        arr: awkward.Array - Input Awkward Array with feature fields\n",
    "        features: tuple - Names of the features to extract, in order\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray of shape (n_objects, len(features)) if flat,\n",
    "        or (n_events, variable-length, len(features)) if nested.\n",
    "    \"\"\"\n",
    "    columns = [ak.to_numpy(arr[feature]) for feature in features]\n",
    "    stacked = np.stack(columns, axis=1)\n",
    "\n",
    "    return stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01d7cfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "import ot\n",
    "\n",
    "def match_showers_to_clusters(\n",
    "    X,\n",
    "    Y,\n",
    "    w_pos=1.0,\n",
    "    w_pt=0.05,\n",
    "    reg=3,\n",
    "    threshold=0.0001,\n",
    "    use_dummy=True,\n",
    "    dummy_cost=5.0,\n",
    "    return_plan=False,\n",
    "    verbose=False,\n",
    "    plot=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Matches particle showers to detector clusters using Sinkhorn Optimal Transport.\n",
    "\n",
    "    Parameters:\n",
    "        X: ndarray (n x d) - Features of true showers (e.g. [x, y, z, pt])\n",
    "        Y: ndarray (m x d) - Features of measured clusters\n",
    "        w_pos: float       - Weight for spatial features\n",
    "        w_pt: float    - Weight for pt feature\n",
    "        threshold: float   - Threshold on transport matrix to consider a match\n",
    "        use_dummy: bool    - Whether to add a dummy cluster to absorb unmatched showers\n",
    "        dummy_cost: float  - Cost assigned to dummy cluster\n",
    "        return_plan: bool  - If True, return the full transport plan P\n",
    "\n",
    "    Returns:\n",
    "        matches: list of (i, j, score) - List of matched shower/cluster pairs with confidence scores\n",
    "        (Optional) P: transport plan matrix (n x m(+1) if dummy used)\n",
    "    \"\"\"\n",
    "    # Normalize features\n",
    "    all_data = np.vstack([X, Y])\n",
    "    mean = all_data.mean(axis=0)\n",
    "    std = all_data.std(axis=0)\n",
    "    X_norm = (X - mean) / std\n",
    "    Y_norm = (Y - mean) / std\n",
    "\n",
    "    # Compute cost matrix\n",
    "    C = (\n",
    "        w_pos * cdist(X_norm[:, :3], Y_norm[:, :3], metric='sqeuclidean') +\n",
    "        w_pt * cdist(X_norm[:, 3:4], Y_norm[:, 3:4], metric='sqeuclidean')\n",
    "    )\n",
    "    # Normalize cost matrix\n",
    "    C /= C.std()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Min:\", C.min(), \"Mean:\", C.mean(), \"Max:\", C.max())\n",
    "        print(\"C range:\", C.min(), C.mean(), C.max(), \" | reg =\", reg)\n",
    "        print(\"exp(-C.max()/reg) =\", np.exp(-C.max() / reg))\n",
    "\n",
    "    # Add dummy cluster if enabled\n",
    "    if use_dummy:\n",
    "        dummy_col = dummy_cost * np.ones((X.shape[0], 1))\n",
    "        C = np.hstack([C, dummy_col])\n",
    "\n",
    "    # Define source and target distributions\n",
    "    a = np.ones(X.shape[0]) / X.shape[0]\n",
    "    b = np.ones(C.shape[1]) / C.shape[1]\n",
    "\n",
    "    # Compute Sinkhorn transport plan\n",
    "    P = ot.sinkhorn(a, b, C, reg=reg)\n",
    "\n",
    "    # print dummy mass\n",
    "    dummy_mass = P[:, -1].sum()\n",
    "    if verbose:\n",
    "        print(f\"Total dummy mass: {dummy_mass:.2f}\")\n",
    "\n",
    "    if plot:\n",
    "        plt.imshow(C, cmap=\"plasma\", aspect=\"auto\")\n",
    "        plt.colorbar()\n",
    "        plt.title(\"Cost Matrix\")\n",
    "\n",
    "    # Extract matches above threshold\n",
    "    matches = []\n",
    "    for i in range(P.shape[0]):\n",
    "        for j in range(P.shape[1]):\n",
    "            if P[i, j] > threshold and (not use_dummy or j < Y.shape[0]):\n",
    "                matches.append((i, j, P[i, j]))\n",
    "\n",
    "    return (matches, P) if return_plan else matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db7299eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import radius_graph\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "\n",
    "#from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def generate_graph_list(clustered_truth_array, calo_array, radius_edges=False):\n",
    "    graph_list = []\n",
    "\n",
    "    for event_idx in range(len(clustered_truth_array)):\n",
    "        matched_clusters = []\n",
    "\n",
    "        try:\n",
    "            # === Get feature arrays for matching\n",
    "            X = extract_features_awkward(clustered_truth_array[event_idx], features=(\"centroid_x\", \"centroid_y\", \"centroid_z\", \"cl_pt\"))\n",
    "            Y = compute_ewc_from_clustered_event(calo_array[event_idx])\n",
    "\n",
    "            if len(X) == 0 or len(Y) == 0:\n",
    "                continue\n",
    "\n",
    "            # === Run Sinkhorn\n",
    "            matches, P = match_showers_to_clusters(X, Y, reg=0.2, threshold=0.011, dummy_cost=0.5, return_plan=True, verbose=False)\n",
    "\n",
    "            for i, j, p_ij in matches:\n",
    "                if j >= Y.shape[0]:\n",
    "                    continue\n",
    "\n",
    "                true_time = np.array(clustered_truth_array[event_idx][\"cl_time\"][i])\n",
    "\n",
    "                cell_x = ak.to_numpy(calo_array[event_idx]['cl_cell_xCells'][j])\n",
    "                cell_y = ak.to_numpy(calo_array[event_idx]['cl_cell_yCells'][j])\n",
    "                cell_z = ak.to_numpy(calo_array[event_idx]['cl_cell_zCells'][j])\n",
    "                cell_E = ak.to_numpy(calo_array[event_idx]['cl_cell_E'][j])\n",
    "                cell_T = ak.to_numpy(calo_array[event_idx]['cl_cell_TimeCells'][j])\n",
    "\n",
    "                # --- Step 2: Build node feature matrix\n",
    "                node_features = torch.tensor(np.stack([cell_x, cell_y, cell_z, cell_E, cell_T], axis=1), dtype=torch.float)\n",
    "\n",
    "                # --- Step 3: Build edges using Radius on (x, y, z)\n",
    "                if radius_edges:\n",
    "                    pos = node_features[:, :3] # Gets (x, y, z) from features\n",
    "                    edge_index = radius_graph(pos, r=150)\n",
    "                else:\n",
    "                    num_nodes = node_features.size(0)\n",
    "                    adj = torch.ones((num_nodes, num_nodes)) - torch.eye(num_nodes)  # full connection minus self-loops\n",
    "                    edge_index, _ = dense_to_sparse(adj)\n",
    "\n",
    "                # --- Step 4: Cluster time (target value)\n",
    "                \"\"\"### Need to be changed for actual target values\"\"\"\n",
    "                cluster_times = np.zeros(len(cell_T))        \n",
    "                target = torch.tensor(true_time, dtype=torch.float)\n",
    "\n",
    "                # --- Step 5: Build graph\n",
    "                graph = Data(x=node_features, edge_index=edge_index, y=target)\n",
    "                graph_list.append(graph)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping event {event_idx}: {e}\")\n",
    "            continue\n",
    "    return graph_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8955f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 - 500\n",
      "Total graphs so far: 3384\n",
      "iteration: 500 - 1000\n",
      "Total graphs so far: 6927\n",
      "iteration: 1000 - 1500\n",
      "Total graphs so far: 10047\n",
      "iteration: 1500 - 2000\n",
      "Total graphs so far: 13407\n",
      "iteration: 2000 - 2500\n",
      "Total graphs so far: 16479\n",
      "iteration: 2500 - 3000\n",
      "Total graphs so far: 19638\n",
      "iteration: 3000 - 3500\n",
      "Total graphs so far: 22425\n",
      "iteration: 3500 - 4000\n",
      "Total graphs so far: 25569\n",
      "iteration: 4000 - 4500\n",
      "Total graphs so far: 28421\n",
      "iteration: 4500 - 5000\n",
      "Total graphs so far: 31078\n",
      "iteration: 5000 - 5500\n",
      "Total graphs so far: 33830\n",
      "iteration: 5500 - 6000\n",
      "Total graphs so far: 37022\n",
      "iteration: 6000 - 6500\n",
      "Skipping inconsistent event at index 434\n",
      "Skipping inconsistent event at index 434\n",
      "Skipping inconsistent event at index 434\n",
      "Skipping event 434: all input arrays must have the same shape\n",
      "Total graphs so far: 39844\n",
      "iteration: 6500 - 7000\n",
      "Total graphs so far: 42936\n",
      "iteration: 7000 - 7500\n",
      "Total graphs so far: 45874\n",
      "iteration: 7500 - 8000\n",
      "Total graphs so far: 49121\n",
      "iteration: 8000 - 8500\n",
      "Total graphs so far: 51710\n",
      "iteration: 8500 - 9000\n",
      "Total graphs so far: 54584\n",
      "iteration: 9000 - 9500\n",
      "Total graphs so far: 57575\n",
      "iteration: 9500 - 10000\n",
      "Total graphs so far: 60282\n"
     ]
    }
   ],
   "source": [
    "FILE_NAME_1 = \"LLP_true_times.caloCells_combined.1.root\"\n",
    "FILE_NAME_2 = \"LLP_true_times.caloCells_combined.2.root\"\n",
    "STEP_SIZE = 500\n",
    "\n",
    "DATA_TREE = \"caloCells\"\n",
    "TRUTH_TREE = \"LLPTruthTree\"\n",
    "\n",
    "# Gets the the latest tree versions\n",
    "calo_tree_1 = uproot.open(FILE_NAME_1 + \":\" + DATA_TREE)\n",
    "truth_tree_1 = uproot.open(FILE_NAME_1 + \":\" + TRUTH_TREE)\n",
    "\n",
    "\"\"\"calo_tree_2 = uproot.open(FILE_NAME_2 + \":\" + DATA_TREE)\n",
    "truth_tree_2 = uproot.open(FILE_NAME_2 + \":\" + TRUTH_TREE)\"\"\"\n",
    "\n",
    "graph_list_all = []  # this will hold all graphs\n",
    "\n",
    "i = 0\n",
    "for calo_batch, truth_batch in zip(\n",
    "    calo_tree_1.iterate(filter_name=[\"cl_cell_*\", \"cl_pt\"], library=\"ak\", step_size=STEP_SIZE),\n",
    "    truth_tree_1.iterate(library=\"ak\", step_size=STEP_SIZE)\n",
    "):\n",
    "    print(f\"iteration: {i*STEP_SIZE} - {(i+1)*STEP_SIZE}\")\n",
    "\n",
    "    clustered_truth_array = create_clustered_truth_array(truth_batch)\n",
    "    graph_list = generate_graph_list(clustered_truth_array, calo_batch)\n",
    "\n",
    "    graph_list_all.extend(graph_list)  # add batch to master list\n",
    "    print(f\"Total graphs so far: {len(graph_list_all)}\")\n",
    "    i+=1\n",
    "\n",
    "\"\"\"for calo_batch, truth_batch in zip(\n",
    "    calo_tree_2.iterate(filter_name=[\"cl_cell_*\", \"cl_pt\"], library=\"ak\", step_size=STEP_SIZE),\n",
    "    truth_tree_2.iterate(library=\"ak\", step_size=STEP_SIZE)\n",
    "):\n",
    "    print(f\"Event iteration: {i*STEP_SIZE} - {(i+1)*STEP_SIZE}\")\n",
    "\n",
    "    clustered_truth_array = create_clustered_truth_array(truth_batch)\n",
    "    graph_list = generate_graph_list(clustered_truth_array, calo_batch)\n",
    "\n",
    "    graph_list_all.extend(graph_list)  # add batch to master list\n",
    "    print(f\"Total graphs so far: {len(graph_list_all)}\")\n",
    "    i+=1\"\"\"\n",
    "\n",
    "torch.save(graph_list_all, \"graph_dataset.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882d8eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "612\n"
     ]
    }
   ],
   "source": [
    "graph_dataset = torch.load(\"graph_dataset.pt\", weights_only=False)\n",
    "print(len(graph_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b316896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "def graphs_are_equal(list1, list2, verbose=False):\n",
    "    if len(list1) != len(list2):\n",
    "        if verbose: print(f\"Length mismatch: {len(list1)} vs {len(list2)}\")\n",
    "        return False\n",
    "\n",
    "    for i, (g1, g2) in enumerate(zip(list1, list2)):\n",
    "        if type(g1) != type(g2):\n",
    "            if verbose: print(f\"Type mismatch at index {i}\")\n",
    "            return False\n",
    "\n",
    "        for attr in ['x', 'edge_index', 'y']:\n",
    "            t1 = getattr(g1, attr, None)\n",
    "            t2 = getattr(g2, attr, None)\n",
    "\n",
    "            if t1 is None and t2 is None:\n",
    "                continue\n",
    "            if t1 is None or t2 is None:\n",
    "                if verbose: print(f\"Missing attribute '{attr}' at index {i}\")\n",
    "                return False\n",
    "            if not torch.equal(t1, t2):\n",
    "                if verbose: print(f\"Mismatch in '{attr}' at index {i}\")\n",
    "                return False\n",
    "\n",
    "    return True\n",
    "\n",
    "same = graphs_are_equal(graph_list_all, graph_dataset, verbose=True)\n",
    "print(\"Graphs match!\" if same else \"Graphs do not match.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d2fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
